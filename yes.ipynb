{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EXP3"
      ],
      "metadata": {
        "id": "etm1jfMRTA6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuqVUV_XcFnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f25da05-967b-4424-aba8-92b0a8ea0182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "!pip install nltk\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize as WT,sent_tokenize as ST,wordpunct_tokenize as WPT,TweetTokenizer as TT\n",
        "\n",
        "sent = \"The quick brown fox jumped. Over the lazy dog! #Dog#Fox\"\n",
        "wt_tokens = WT(sent)\n",
        "st_tokens = ST(sent)\n",
        "wpt_tokens = WPT(sent)\n",
        "tt_tokens = TT().tokenize(sent)\n",
        "print(wt_tokens)\n",
        "print(st_tokens)\n",
        "print(wpt_tokens)\n",
        "print(tt_tokens)"
      ],
      "metadata": {
        "id": "vSa9NZDwdGzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa0df0d-6231-4cba-bdcc-b9d4fd87c332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumped', '.', 'Over', 'the', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox']\n",
            "['The quick brown fox jumped.', 'Over the lazy dog!', '#Dog#Fox']\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', '.', 'Over', 'the', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox']\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', '.', 'Over', 'the', 'lazy', 'dog', '!', '#Dog', '#Fox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop word removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize as WT\n",
        "\n",
        "print(stopwords.words('english'))\n",
        "StopWords=stopwords.words('english')\n",
        "\n",
        "sent = \"The quick brown fox jumped. Over a lazy dog! #Dog#Fox. Dancing\"\n",
        "wt_tokens = WT(sent)\n",
        "\n",
        "after_removing_stopwords = [ w for w in wt_tokens if w.lower() not in StopWords]\n",
        "print(after_removing_stopwords)"
      ],
      "metadata": {
        "id": "6lQJTuDDgtzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda1bb0f-e6c2-418a-ede4-1f4cc87ffc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['quick', 'brown', 'fox', 'jumped', '.', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox', '.', 'Dancing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "PS = PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [PS.stem(w) for w in after_removing_stopwords]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "id": "LUBPRhIYvLYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3511528c-7c38-402b-974f-4accf23bbbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jump', '.', 'lazi', 'dog', '!', '#', 'dog', '#', 'fox', '.', 'danc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmetization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "LM = WordNetLemmatizer()\n",
        "lemm_words = [LM.lemmatize(w) for w in after_removing_stopwords]\n",
        "print(lemm_words)"
      ],
      "metadata": {
        "id": "3_ebcwC7vfDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0c0f89-222f-4439-eff4-f72b3adc289c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jumped', '.', 'lazy', 'dog', '!', '#', 'Dog', '#', 'Fox', '.', 'Dancing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP4"
      ],
      "metadata": {
        "id": "PgsLt4YkTKQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sent = \"The quick brown fox jumped over the lazy dog.\"\n",
        "doc = nlp(sent)\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.text} => {token.pos_}\")"
      ],
      "metadata": {
        "id": "gt0KlEt_TNuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dependency parsing\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sent = \"The quick brown fox jumped over the lazy dog.\"\n",
        "doc = nlp(sent)\n",
        "\n",
        "for token in doc :\n",
        "  print(f\"{token.text} {token.dep_} > {token.head.text}\")"
      ],
      "metadata": {
        "id": "dVzg75pDVJyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#named entity recognition\n",
        "import spacy\n",
        "\n",
        "# Load English NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "# Process the text with SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "X0WFKAPLu4B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "MQzvnIvzu1Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP 5"
      ],
      "metadata": {
        "id": "UyRoYNKbYOdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "corpus_reshaped = [[doc] for doc in corpus]\n",
        "\n",
        "ohe = OneHotEncoder(sparse = False)\n",
        "encoded = ohe.fit_transform(corpus_reshaped)\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "GvltCEJfYQrU",
        "outputId": "37e314cf-02b0-40b5-d30d-d16d8381df0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "\n",
        "vec = CountVectorizer()\n",
        "x=vec.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(vec.vocabulary_) # shows the indices"
      ],
      "metadata": {
        "id": "ObPYQbDed8YH",
        "outputId": "035df3c9-83a5-4b82-e672-862d871f49b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 0 0 1 0]\n",
            " [1 1 0 0 1 0]\n",
            " [0 1 1 0 0 1]\n",
            " [0 0 1 1 1 0]]\n",
            "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bag of N grams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "\n",
        "vec = CountVectorizer(ngram_range=(1,2))\n",
        "x=vec.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(vec.vocabulary_) # shows the indices"
      ],
      "metadata": {
        "id": "jfHwx4neffpw",
        "outputId": "a9dc3d69-47bc-42e9-d9b3-79bf036524da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 1 1 0 0 0 0 0 1 0 0 0]\n",
            " [1 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 0 0 1 0 1 1 0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1 1 0 1 1 0 1 0]]\n",
            "{'dog': 3, 'bites': 0, 'man': 10, 'dog bites': 4, 'bites man': 2, 'man bites': 11, 'bites dog': 1, 'eats': 6, 'meat': 13, 'dog eats': 5, 'eats meat': 8, 'food': 9, 'man eats': 12, 'eats food': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "tfidf = TfidfVectorizer()\n",
        "x = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(tfidf.vocabulary_)"
      ],
      "metadata": {
        "id": "90Cqd7-SoVqN",
        "outputId": "d5e978b2-45fc-4640-d7da-234e84d191e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "p7nhO7RGt-3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embeddings\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "corpus = [\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "tokenized_corpus = [doc.split() for doc in corpus]\n",
        "\n",
        "model = Word2Vec(sentences = tokenized_corpus ,vector_size = 100, window=5,min_count=1,workers=4)\n",
        "def embeddings(doc):\n",
        "  emb = [model.wv[word] for word in doc if word in model.wv]\n",
        "  if emb:\n",
        "    return np.mean(emb,axis=0)\n",
        "  else:\n",
        "    return np.zeros(model.vector_size)\n",
        "\n",
        "x = np.array([embeddings(doc) for doc in tokenized_corpus])\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "id": "Hcymp-ZWo6wO",
        "outputId": "3028dceb-c1e7-455b-9550-a68ed80cc09a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-5.7995315e-03  4.4005080e-03  3.3651907e-03  4.2613116e-03\n",
            "   9.2253304e-04 -5.7932665e-03  3.4358669e-03  7.3200786e-03\n",
            "  -5.9668423e-04 -5.8158967e-03  5.4508769e-03 -1.7435373e-03\n",
            "  -2.0568350e-03  2.4716950e-03  2.3174530e-03  1.0864957e-03\n",
            "   6.1673387e-03  1.0535377e-03 -6.7345961e-03 -1.0620216e-03\n",
            "   3.7771116e-03 -5.4909306e-04  8.2200086e-03  1.3285212e-04\n",
            "   1.1138282e-03  6.7142653e-04  9.2486465e-05  4.6797390e-03\n",
            "  -2.4128084e-03  1.0685941e-03  1.0422996e-03 -3.0377072e-03\n",
            "   5.9951021e-04 -5.0345589e-03  1.8136505e-03  3.2225230e-03\n",
            "   8.0758883e-03 -1.9352456e-03  6.0329461e-05  4.7613536e-03\n",
            "   5.7476730e-04  1.0613020e-03 -3.8777019e-03 -2.5172802e-03\n",
            "   1.3959208e-03  3.4761357e-03  3.8277949e-04  1.8962369e-03\n",
            "   1.1215905e-03  2.8843472e-03  1.3120301e-03 -1.6461100e-03\n",
            "  -4.6422952e-03 -1.7906037e-03  4.7048461e-04  1.2636752e-03\n",
            "   5.1752334e-03  3.0605046e-03  4.4141375e-04  4.2727385e-03\n",
            "  -3.2778904e-03  2.8309457e-03 -3.6010580e-04 -4.6386220e-03\n",
            "   2.1350102e-03  6.7525153e-04  2.0257656e-03  2.6224020e-03\n",
            "   7.7677291e-04 -2.5929790e-04  2.5084179e-03 -1.4431704e-03\n",
            "   9.2028471e-04  2.4273768e-03  4.4221725e-04 -3.6317119e-03\n",
            "  -1.6578013e-03  1.2235156e-03 -1.0680107e-03 -3.4348611e-03\n",
            "  -5.1607587e-03  4.2580054e-03  4.8810714e-03  1.0768159e-03\n",
            "  -1.9952154e-03 -9.8737364e-04  5.0354009e-03 -3.1552620e-03\n",
            "  -8.5925852e-04 -2.4742624e-03 -1.5689767e-03 -8.1668299e-04\n",
            "   1.6251210e-03 -3.4089026e-03  5.5281208e-03  2.4217607e-03\n",
            "   7.6768425e-04 -5.6268875e-03 -3.7049039e-03  1.7828498e-03]\n",
            " [-5.7995305e-03  4.4005080e-03  3.3651907e-03  4.2613116e-03\n",
            "   9.2253304e-04 -5.7932674e-03  3.4358669e-03  7.3200795e-03\n",
            "  -5.9668423e-04 -5.8158967e-03  5.4508769e-03 -1.7435373e-03\n",
            "  -2.0568350e-03  2.4716950e-03  2.3174530e-03  1.0864957e-03\n",
            "   6.1673387e-03  1.0535377e-03 -6.7345966e-03 -1.0620216e-03\n",
            "   3.7771116e-03 -5.4909306e-04  8.2200086e-03  1.3285223e-04\n",
            "   1.1138282e-03  6.7142653e-04  9.2486385e-05  4.6797390e-03\n",
            "  -2.4128084e-03  1.0685942e-03  1.0422996e-03 -3.0377072e-03\n",
            "   5.9951021e-04 -5.0345589e-03  1.8136505e-03  3.2225230e-03\n",
            "   8.0758883e-03 -1.9352456e-03  6.0329523e-05  4.7613536e-03\n",
            "   5.7476730e-04  1.0613020e-03 -3.8777019e-03 -2.5172802e-03\n",
            "   1.3959209e-03  3.4761354e-03  3.8277949e-04  1.8962367e-03\n",
            "   1.1215905e-03  2.8843477e-03  1.3120301e-03 -1.6461102e-03\n",
            "  -4.6422952e-03 -1.7906037e-03  4.7048469e-04  1.2636753e-03\n",
            "   5.1752334e-03  3.0605048e-03  4.4141369e-04  4.2727385e-03\n",
            "  -3.2778902e-03  2.8309457e-03 -3.6010580e-04 -4.6386220e-03\n",
            "   2.1350102e-03  6.7525153e-04  2.0257656e-03  2.6224020e-03\n",
            "   7.7677285e-04 -2.5929790e-04  2.5084179e-03 -1.4431701e-03\n",
            "   9.2028460e-04  2.4273768e-03  4.4221722e-04 -3.6317117e-03\n",
            "  -1.6578016e-03  1.2235157e-03 -1.0680108e-03 -3.4348613e-03\n",
            "  -5.1607587e-03  4.2580054e-03  4.8810714e-03  1.0768158e-03\n",
            "  -1.9952154e-03 -9.8737364e-04  5.0354013e-03 -3.1552620e-03\n",
            "  -8.5925846e-04 -2.4742624e-03 -1.5689767e-03 -8.1668299e-04\n",
            "   1.6251210e-03 -3.4089026e-03  5.5281208e-03  2.4217607e-03\n",
            "   7.6768425e-04 -5.6268875e-03 -3.7049039e-03  1.7828498e-03]\n",
            " [-5.7514342e-03  2.9579282e-03 -8.3226990e-04 -1.6514994e-03\n",
            "   1.9019647e-03 -7.7299854e-05  6.2180310e-04  4.1316352e-03\n",
            "  -5.8858488e-03 -2.2807133e-03 -3.3493862e-03 -4.6068332e-03\n",
            "  -1.1799670e-03 -5.4240756e-04  2.6998150e-03 -2.4043871e-03\n",
            "   1.9736513e-03  3.9273384e-03 -4.8638512e-03 -2.4683594e-03\n",
            "  -4.5585171e-03 -3.3724215e-03  4.2546927e-03 -3.5676176e-03\n",
            "   2.5043723e-03 -1.8657680e-03  2.2034512e-03  2.9476048e-05\n",
            "  -2.2710648e-03  5.7317293e-03  7.0609357e-03 -6.8513714e-03\n",
            "  -4.1744816e-03 -5.4375790e-03 -1.5764531e-03  3.4901388e-03\n",
            "   4.1540312e-03  4.8230793e-03  5.3474992e-03  1.7161345e-03\n",
            "   3.9525940e-03 -4.2056371e-03 -5.7750284e-03  1.3908135e-03\n",
            "   3.1743879e-05  3.0690935e-03  2.2600831e-03  1.0815440e-03\n",
            "  -1.1845726e-03 -2.8911393e-04  3.9998679e-03 -6.8390183e-03\n",
            "   1.9255809e-04  1.4004252e-03 -6.5490534e-03  1.6310184e-03\n",
            "   5.6900568e-03 -1.8182914e-03 -4.7136280e-03 -1.2483750e-03\n",
            "   1.3841981e-03 -4.0452948e-04  4.2955931e-03 -3.9048993e-03\n",
            "   3.0252850e-05  5.8133849e-03  5.3813192e-03  6.8505801e-04\n",
            "  -2.2954566e-03  1.0294210e-03  8.8484143e-04  8.6321170e-04\n",
            "   5.8936072e-03  4.7288723e-03 -1.5784519e-03  3.3173936e-03\n",
            "  -2.2704697e-03  1.5675082e-03  8.5764175e-04  6.6628832e-05\n",
            "  -2.5465293e-04 -3.7861587e-03  7.4856682e-04 -5.3945570e-03\n",
            "   5.4709689e-04 -5.5029900e-03 -3.2669229e-03 -2.6463021e-03\n",
            "   3.0541348e-03 -5.7126937e-04  1.8836915e-03  4.3552113e-03\n",
            "   1.0557011e-03 -4.9981833e-03  5.0454731e-03  4.3814406e-03\n",
            "   3.2125653e-03 -3.5242049e-03  5.5516191e-04 -3.0635681e-03]\n",
            " [-2.5272444e-03  1.5183970e-03 -2.9621504e-03  1.7959969e-03\n",
            "   6.9472188e-04  2.0209707e-03  1.3280390e-03  4.5730001e-03\n",
            "  -5.8147148e-03  3.2223060e-03 -1.1882312e-03 -1.6931444e-05\n",
            "  -1.1485516e-03  3.1542608e-03 -7.8215869e-04 -4.0463605e-03\n",
            "   1.6425406e-03  5.5232341e-03 -3.6483016e-03 -6.5198722e-03\n",
            "   3.3960887e-04 -2.7628317e-03  3.5965799e-03  4.9482455e-04\n",
            "   3.8545609e-03 -2.4832543e-03  2.7415513e-03  5.5027083e-03\n",
            "  -7.5592943e-03  2.0240311e-04  1.6096517e-03 -2.0421371e-03\n",
            "   1.0853881e-03 -6.3725538e-03 -8.2584779e-04 -3.2607978e-03\n",
            "   1.5607787e-03  2.8778412e-04  2.3836676e-04 -1.4081002e-03\n",
            "  -4.4715772e-03 -3.0683044e-05 -6.4608981e-03  3.8141694e-03\n",
            "   3.3788818e-03 -8.9326873e-04 -9.4115111e-04  2.3739096e-03\n",
            "   1.7895642e-03  3.6029253e-04 -8.9784403e-04  2.0653598e-03\n",
            "  -4.1762739e-04 -3.8179520e-03 -6.7134690e-04 -5.9048869e-03\n",
            "  -2.3422139e-03 -6.7307441e-03 -3.6776920e-03 -2.7440846e-04\n",
            "   2.5208213e-03 -3.1939882e-03  2.2527503e-03 -1.8033896e-03\n",
            "  -4.0474609e-03  3.6417171e-03  5.5742580e-03  6.7244605e-03\n",
            "  -4.6849647e-03  5.7216398e-03  1.2745354e-03  6.4877481e-03\n",
            "  -9.7959848e-05 -1.2786152e-03  1.4057889e-03  5.2758362e-03\n",
            "   4.8931804e-03  3.9183185e-03  7.0463755e-04 -6.0976869e-03\n",
            "  -2.5483305e-03 -4.4236295e-03  3.3672201e-05  1.3419170e-03\n",
            "  -3.7984636e-03 -4.0047201e-03  3.2239731e-03 -3.3842896e-03\n",
            "  -1.3688264e-03  7.0885691e-04  8.6565269e-05  3.9326012e-04\n",
            "   5.9779827e-03 -2.7053673e-03  6.7422981e-03  7.9707772e-04\n",
            "   1.0572380e-03 -5.7037035e-04  3.3008317e-03  7.9526234e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP2"
      ],
      "metadata": {
        "id": "HESpGubh9YoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize as WT\n",
        "\n",
        "sent=\"hello how are you doing and what do you do\"\n",
        "nltk.download('punkt')\n",
        "tokens=WT(sent)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "NKBRR9y29bPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEXTBLOB\n",
        "from textblob import TextBlob\n",
        "text=\"I love using this product. It's fantastic!\"\n",
        "blob=TextBlob(text)\n",
        "sentiment=blob.sentiment.polarity\n",
        "print(sentiment)"
      ],
      "metadata": {
        "id": "3v8Ngnof99y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "id": "bC5dq9KZziqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FLAIR\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# Load NER model\n",
        "tagger = SequenceTagger.load('ner')\n",
        "\n",
        "# Example text\n",
        "text = \"Flair is a simple and powerful NLP library.\"\n",
        "\n",
        "# Tokenize the text\n",
        "sentence = Sentence(text)\n",
        "\n",
        "# Tag the text with NER\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# Get named entities\n",
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "metadata": {
        "id": "thbUIC5e0F8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624996ed-1d0d-4138-84a2-7bd5ffc57ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-15 05:37:50,115 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
            "Span[0:1]: \"Flair\" → ORG (0.2899)\n",
            "Span[6:7]: \"NLP\" → MISC (0.6945)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SPACY\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "sent=\"a quick brown fox jumped.Over the lazy dog.\"\n",
        "doc=nlp(sent)\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.text}=>{token.pos_}\")"
      ],
      "metadata": {
        "id": "DTwTGyk617oS",
        "outputId": "ec2b5901-80a7-4ccc-f463-17fb0ccebd5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=>DET\n",
            "quick=>ADJ\n",
            "brown=>ADJ\n",
            "fox=>NOUN\n",
            "jumped=>VERB\n",
            ".=>PUNCT\n",
            "Over=>ADP\n",
            "the=>DET\n",
            "lazy=>ADJ\n",
            "dog=>NOUN\n",
            ".=>PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SKLEARN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus=[\n",
        "    'dog bites man',\n",
        "    'man bites dog',\n",
        "    'dog eats meat',\n",
        "    'man eats food'\n",
        "]\n",
        "vec=CountVectorizer()\n",
        "x=vec.fit_transform(corpus)\n",
        "\n",
        "print(x.toarray())\n",
        "print(vec.vocabulary_)"
      ],
      "metadata": {
        "id": "hhjzzvtf19hy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}